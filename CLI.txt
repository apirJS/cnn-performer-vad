
python inference.py --checkpoint "D:\belajar\audio\vad\lightning_logs\lightning_logs\version_2\checkpoints\07-0.1605-0.9331.ckpt" --audio "D:\belajar\audio\vad\datasets\prepared\test\pos\pos_music_000362.wav" --threshold 0.5 --device cuda


python cli.py prepare --root datasets --splits train --n_pos 20000 --duration_range 1.0 12.0 --use_silero_vad && python cli.py prepare --root datasets --splits val --n_pos 2500 --duration_range 1.0 12.0 --use_silero_vad && python cli.py prepare --root datasets --splits test --n_pos 3500 --duration_range 1.0 12.0 --use_silero_vad

python cli.py train --accumulate_grad_batches 2 --ckpt_path "D:\belajar\audio\vad\lightning_logs\lightning_logs\version_6\checkpoints\00-0.0218-0.9104.ckpt" --vad_root datasets --train_manifest "D:\belajar\audio\vad\datasets\manifest_train.csv" --val_manifest "D:\belajar\audio\vad\datasets\manifest_val.csv" --test_manifest "D:\belajar\audio\vad\datasets\manifest_test.csv" --test_after_training --use_mel_cache --export_model --export_quantized --export_path "D:\belajar\audio\vad\models\model.pt"

python cli.py evaluate --model_path "D:\belajar\audio\vad\lightning_logs\lightning_logs\version_8\checkpoints\14-0.0152-0.9280.ckpt" --test_manifest "D:\belajar\audio\vad\datasets\manifest_test.csv" --output_dir "D:\belajar\audio\vad\evaluation_results" --pytorch_model "D:\belajar\audio\vad\models\model.pt" --quantized_model "D:\belajar\audio\vad\models\model_quantized.pt" --compare_models --boundary_analysis --two_stage_eval

python cli.py evaluate --model_path "D:\belajar\audio\vad\lightning_logs\finetuning\lightning_logs\version_1\checkpoints\18-0.0164-0.9327.ckpt" --test_manifest "D:\belajar\audio\vad\datasets\manifest_test.csv" --output_dir "D:\belajar\audio\vad\evaluation_results" --pytorch_model "D:\belajar\audio\vad\models\vad.pt" --boundary_analysis --two_stage_eval --smoothing_window_ms 50 --min_segment_duration_ms 150 --max_gap_duration_ms 150 --quantized_model "D:\belajar\audio\vad\models\vad_quantized.pt" --compare_models

python train.py --ckpt_path "D:\belajar\audio\vad\lightning_logs\finetuning\lightning_logs\version_1\checkpoints\18-0.0164-0.9327.ckpt" --train_manifest "datasets/manifest_train.csv" --val_manifest "datasets/manifest_val.csv" --test_manifest "datasets/manifest_test.csv" --export_model --max_epochs 19 --export_path "D:\belajar\audio\vad\models\vad.pt"

Basic Dynamic Quantization
python quantization.py --model_path models/vad_model.pt --method dynamic

Dynamic Quantization with Conv2D Layers
python quantization.py --model_path "D:\belajar\audio\vad\models\vad.pt" --method dynamic --quantize_conv

Static Quantization with Calibration
python quantization.py --model_path models/vad_model.pt --method static --calibration_dataset datasets/manifest_val.csv

With Benchmarking
python quantization.py --model_path models/vad_model.pt --method dynamic --benchmark










FINETUNING

Step 1: Run evaluation with optimized parameters
python cli.py evaluate --model_path "D:\belajar\audio\vad\lightning_logs\lightning_logs\version_8\checkpoints\14-0.0152-0.9280.ckpt" --test_manifest "D:\belajar\audio\vad\datasets\manifest_test.csv" --output_dir "D:\belajar\audio\vad\optimized_results" --boundary_analysis --smoothing_window_ms 50 --min_segment_duration_ms 150 --max_gap_duration_ms 150

Step 2: Test different post-processing parameters
python test_boundary_settings.py

Step 3: Fine-tune with boundary-focused loss (if needed)
python boundary_finetune.py

Step 4: Compare original vs. fine-tuned model
python compare_models.py

Step 5: Export and quantize the best model
python cli.py train --ckpt_path "D:\belajar\audio\vad\lightning_logs\lightning_logs\version_8\checkpoints\14-0.0152-0.9280.ckpt" --export_model --export_quantized --export_path "D:\belajar\audio\vad\models\model_final.pt"

Further Optimizations
If segment F1 remains below 0.75, try adjusting the hysteresis thresholds in smooth_predictions_with_hysteresis
Consider experimenting with the boundary weight in BoundaryFocalLoss (try values from 2.0-4.0)
If fine-tuning, you can add callbacks to monitor segment F1 rather than frame F1
This implementation approach gives you a systematic way to test and implement each improvement, from post-processing optimizations to model fine-tuning, with clear steps to measure the impact of each change.


IF TRAIN FROM SCRATCH, CONSIDER THIS:
And add these parameters when running training:
python cli.py train --boundary_focused_loss --lr 5e-4 --max_epochs 25 --batch_size 32 --accumulate_grad_batches 2 --time_mask_max 60 --freq_mask_max 15

ONNX
python export_vad_onnx.py --ckpt "D:\belajar\audio\vad\models\vad.pt" --output "D:/belajar/audio/vad/models/vad.onnx" --n-mels 80 --dummy-frames 1000 --use-dynamo
python test.py 



python export_vad_onnx.py --ckpt path/to/quantized_model.ckpt --output vad_quant.onnx --quantized


EVAL
--boundary_analysis --smoothing_window_ms 30 --min_segment_duration_ms 130 --max_gap_duration_ms 70
python cli.py evaluate --model_path "D:\belajar\audio\vad\lightning_logs\finetuning\lightning_logs\version_1\checkpoints\18-0.0164-0.9327.ckpt" --test_manifest "D:\belajar\audio\vad\datasets\manifest_test.csv" --output_dir "D:\belajar\audio\vad\evaluation_results" --pytorch_model "D:\belajar\audio\vad\models\vad.pt" --boundary_analysis --two_stage_eval --smoothing_window_ms 30 --min_segment_duration_ms 130 --max_gap_duration_ms 70 --quantized_model "D:\belajar\audio\vad\models\vad_quantized.pt" --compare_models